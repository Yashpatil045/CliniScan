{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVY3X7vDJZ_w"
      },
      "source": [
        "#DATA Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qZGiLRHPQylM",
        "outputId": "34827769-93d4-4537-f594-109ea0536f5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.20.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2025.10.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ns4Mn8kbuRfa"
      },
      "source": [
        "**Calling Data from the Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MQ6yj_LZEGet",
        "outputId": "519edca6-02cf-49f8-fb6d-0358ce835dba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieving folder contents\n",
            "Processing file 10INd9aPTijcKDQG_0Egi-6F2jLBx5121 Data_Entry_2017.csv.zip\n",
            "Processing file 1mkfHerP-4Dsx9F-Ur1Xko1f9d30BXrwf images_001.zip\n",
            "Processing file 1LvnZLQindnQYLopva-Zh_dEMwSXhXCB2 images_002.zip\n",
            "Processing file 1HcsZLwP3YV_CwbHJcnCFK7pHnYccC554 images_003.zip\n",
            "Processing file 1MD3dkJrt_cb6SEJ_LvjYPbzTKXkLCkpW kaggle.json\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=10INd9aPTijcKDQG_0Egi-6F2jLBx5121\n",
            "To: /content/infosys Data/Data_Entry_2017.csv.zip\n",
            "100% 946k/946k [00:00<00:00, 131MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1mkfHerP-4Dsx9F-Ur1Xko1f9d30BXrwf\n",
            "From (redirected): https://drive.google.com/uc?id=1mkfHerP-4Dsx9F-Ur1Xko1f9d30BXrwf&confirm=t&uuid=a490bcb3-69d0-4e2e-a986-00014643a8e3\n",
            "To: /content/infosys Data/images_001.zip\n",
            "100% 2.01G/2.01G [00:38<00:00, 52.4MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1LvnZLQindnQYLopva-Zh_dEMwSXhXCB2\n",
            "From (redirected): https://drive.google.com/uc?id=1LvnZLQindnQYLopva-Zh_dEMwSXhXCB2&confirm=t&uuid=936ca32a-8bb9-4921-983e-1ece4a1a29e4\n",
            "To: /content/infosys Data/images_002.zip\n",
            "100% 3.95G/3.95G [01:42<00:00, 38.6MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1HcsZLwP3YV_CwbHJcnCFK7pHnYccC554\n",
            "From (redirected): https://drive.google.com/uc?id=1HcsZLwP3YV_CwbHJcnCFK7pHnYccC554&confirm=t&uuid=9a59f702-da4a-46ce-9a79-a4a647be3844\n",
            "To: /content/infosys Data/images_003.zip\n",
            "  0% 4.72M/3.93G [00:00<04:38, 14.1MB/s]"
          ]
        }
      ],
      "source": [
        "# Replace with your folder ID\n",
        "\n",
        "!gdown --folder https://drive.google.com/drive/folders/$folder_id\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbO8GBf4urbe"
      },
      "source": [
        "***Unzip the zip Files***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hk_Cl7HuEGb8",
        "outputId": "b03ca6bb-1d27-4481-fd03-dc1276634bf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /content/infosys Data/Data_Entry_2017.csv.zip ...\n",
            "Extracting /content/infosys Data/images_001.zip ...\n",
            "Extracting /content/infosys Data/images_002.zip ...\n",
            "Extracting /content/infosys Data/images_003.zip ...\n",
            "Extraction complete!\n",
            "Sample files: ['Data_Entry_2017.csv', 'images']\n"
          ]
        }
      ],
      "source": [
        "import zipfile, os\n",
        "\n",
        "# define paths\n",
        "base_path = \"/content/infosys Data\"\n",
        "extract_path = \"/content/nih_chest_xray_data\"\n",
        "\n",
        "zip_files = [\n",
        "    f\"{base_path}/Data_Entry_2017.csv.zip\",\n",
        "    f\"{base_path}/images_001.zip\",\n",
        "    f\"{base_path}/images_002.zip\",\n",
        "    f\"{base_path}/images_003.zip\",\n",
        "]\n",
        "\n",
        "# extract all zips\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "for z in zip_files:\n",
        "    print(f\"Extracting {z} ...\")\n",
        "    with zipfile.ZipFile(z, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Extraction complete!\")\n",
        "print(\"Sample files:\", os.listdir(extract_path)[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7QdsBZquvyz"
      },
      "source": [
        "**Data import and read**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "QGGmsaNOEGZa",
        "outputId": "f5e6e8a9-dc01-4871-e77a-2c2c52b49d35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV shape: (112120, 12)\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-fb3f5574-9f2c-43a7-8323-cadfddf1bf04\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image Index</th>\n",
              "      <th>Finding Labels</th>\n",
              "      <th>Follow-up #</th>\n",
              "      <th>Patient ID</th>\n",
              "      <th>Patient Age</th>\n",
              "      <th>Patient Gender</th>\n",
              "      <th>View Position</th>\n",
              "      <th>OriginalImage[Width</th>\n",
              "      <th>Height]</th>\n",
              "      <th>OriginalImagePixelSpacing[x</th>\n",
              "      <th>y]</th>\n",
              "      <th>Unnamed: 11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00000001_000.png</td>\n",
              "      <td>Cardiomegaly</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>58</td>\n",
              "      <td>M</td>\n",
              "      <td>PA</td>\n",
              "      <td>2682</td>\n",
              "      <td>2749</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.143</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>00000001_001.png</td>\n",
              "      <td>Cardiomegaly|Emphysema</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>58</td>\n",
              "      <td>M</td>\n",
              "      <td>PA</td>\n",
              "      <td>2894</td>\n",
              "      <td>2729</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.143</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00000001_002.png</td>\n",
              "      <td>Cardiomegaly|Effusion</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>58</td>\n",
              "      <td>M</td>\n",
              "      <td>PA</td>\n",
              "      <td>2500</td>\n",
              "      <td>2048</td>\n",
              "      <td>0.168</td>\n",
              "      <td>0.168</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00000002_000.png</td>\n",
              "      <td>No Finding</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>81</td>\n",
              "      <td>M</td>\n",
              "      <td>PA</td>\n",
              "      <td>2500</td>\n",
              "      <td>2048</td>\n",
              "      <td>0.171</td>\n",
              "      <td>0.171</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00000003_000.png</td>\n",
              "      <td>Hernia</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>81</td>\n",
              "      <td>F</td>\n",
              "      <td>PA</td>\n",
              "      <td>2582</td>\n",
              "      <td>2991</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.143</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fb3f5574-9f2c-43a7-8323-cadfddf1bf04')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fb3f5574-9f2c-43a7-8323-cadfddf1bf04 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fb3f5574-9f2c-43a7-8323-cadfddf1bf04');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-325420e0-0882-4cb9-b2f4-cac671a5bdef\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-325420e0-0882-4cb9-b2f4-cac671a5bdef')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-325420e0-0882-4cb9-b2f4-cac671a5bdef button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "        Image Index          Finding Labels  Follow-up #  Patient ID  \\\n",
              "0  00000001_000.png            Cardiomegaly            0           1   \n",
              "1  00000001_001.png  Cardiomegaly|Emphysema            1           1   \n",
              "2  00000001_002.png   Cardiomegaly|Effusion            2           1   \n",
              "3  00000002_000.png              No Finding            0           2   \n",
              "4  00000003_000.png                  Hernia            0           3   \n",
              "\n",
              "   Patient Age Patient Gender View Position  OriginalImage[Width  Height]  \\\n",
              "0           58              M            PA                 2682     2749   \n",
              "1           58              M            PA                 2894     2729   \n",
              "2           58              M            PA                 2500     2048   \n",
              "3           81              M            PA                 2500     2048   \n",
              "4           81              F            PA                 2582     2991   \n",
              "\n",
              "   OriginalImagePixelSpacing[x     y]  Unnamed: 11  \n",
              "0                        0.143  0.143          NaN  \n",
              "1                        0.143  0.143          NaN  \n",
              "2                        0.168  0.168          NaN  \n",
              "3                        0.171  0.171          NaN  \n",
              "4                        0.143  0.143          NaN  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "csv_path = \"/content/nih_chest_xray_data/Data_Entry_2017.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "print(\"CSV shape:\", df.shape)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_S0jfi-1noM",
        "outputId": "341d4ba1-dede-4b54-b83d-9e88e2f4b278"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 112120 entries, 0 to 112119\n",
            "Data columns (total 12 columns):\n",
            " #   Column                       Non-Null Count   Dtype  \n",
            "---  ------                       --------------   -----  \n",
            " 0   Image Index                  112120 non-null  object \n",
            " 1   Finding Labels               112120 non-null  object \n",
            " 2   Follow-up #                  112120 non-null  int64  \n",
            " 3   Patient ID                   112120 non-null  int64  \n",
            " 4   Patient Age                  112120 non-null  int64  \n",
            " 5   Patient Gender               112120 non-null  object \n",
            " 6   View Position                112120 non-null  object \n",
            " 7   OriginalImage[Width          112120 non-null  int64  \n",
            " 8   Height]                      112120 non-null  int64  \n",
            " 9   OriginalImagePixelSpacing[x  112120 non-null  float64\n",
            " 10  y]                           112120 non-null  float64\n",
            " 11  Unnamed: 11                  0 non-null       float64\n",
            "dtypes: float64(3), int64(5), object(4)\n",
            "memory usage: 10.3+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZbuy0lGvRaA"
      },
      "source": [
        "# Data Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2sR0MUI2qX2"
      },
      "source": [
        "**Resizing the Images from 1024 x 1024 to 224 X 224**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdCYndG02mhz",
        "outputId": "586a6197-d366-4b13-f84d-d22a9550ac51"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 24999/24999 [08:23<00:00, 49.68it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Successfully resized 24999 images to 224×224 and saved to: /content/nih_chest_xray_data/resized_224\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Paths\n",
        "source_folder = \"/content/nih_chest_xray_data/images\"        # folder with original images\n",
        "destination_folder = \"/content/nih_chest_xray_data/resized_224\"  # folder to save resized images\n",
        "\n",
        "# Create destination folder if not exists\n",
        "os.makedirs(destination_folder, exist_ok=True)\n",
        "\n",
        "# Resize all images to 224×224\n",
        "count = 0\n",
        "for filename in tqdm(os.listdir(source_folder)):\n",
        "    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "        img_path = os.path.join(source_folder, filename)\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is not None:\n",
        "            resized = cv2.resize(img, (224, 224))\n",
        "            cv2.imwrite(os.path.join(destination_folder, filename), resized)\n",
        "            count += 1\n",
        "\n",
        "print(f\"\\nSuccessfully resized {count} images to 224×224 and saved to: {destination_folder}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "izERoXEOMU8b"
      },
      "outputs": [],
      "source": [
        "# import shutil\n",
        "\n",
        "# # Zip the resized folder\n",
        "# shutil.make_archive('/content/resized_224', 'zip', '/content/nih_chest_xray_data/resized_224')\n",
        "\n",
        "# # Download the zip file\n",
        "# from google.colab import files\n",
        "# files.download('/content/resized_224.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "527009cf"
      },
      "source": [
        "**Develop scripts for converting medical image annotations from CSV to COCO format.The annotation conversion script should handle the \"Data_Entry_2017.csv\" file.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQvxoQj5kF8w"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# subset of images\n",
        "image_dir = \"/content/nih_chest_xray_data/resized_224\"  # Corrected path\n",
        "\n",
        "output_json = \"/content/chest_xray_coco_cleaned.json\"\n",
        "\n",
        "# Filtering CSV\n",
        "# finding all image files in the image_dir\n",
        "valid_images = set()\n",
        "for root, _, files in os.walk(image_dir):\n",
        "    for file in files:\n",
        "        if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            valid_images.add(file)\n",
        "\n",
        "df = df[df['Image Index'].isin(valid_images)]\n",
        "print(f\"Filtered CSV shape: {df.shape}\")\n",
        "\n",
        "# Parse finding labels\n",
        "def parse_annotations(df):\n",
        "    annotations = {}\n",
        "    for _, row in df.iterrows():\n",
        "        annotations[row['Image Index']] = row['Finding Labels'].split('|')\n",
        "    return annotations\n",
        "\n",
        "image_annotations = parse_annotations(df)\n",
        "\n",
        "# Creating COCO structure\n",
        "unique_labels = sorted({label for labels in image_annotations.values() for label in labels})\n",
        "label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
        "\n",
        "coco = {\"images\": [], \"annotations\": [], \"categories\": []}\n",
        "\n",
        "for label, i in label_to_id.items():\n",
        "    coco[\"categories\"].append({\n",
        "        \"id\": i,\n",
        "        \"name\": label,\n",
        "        \"supercategory\": \"chest xray\"\n",
        "    })\n",
        "\n",
        "# Adding image and annotation entries\n",
        "ann_id = 0\n",
        "for image_name, labels in tqdm(image_annotations.items(), desc=\"Converting to COCO\"):\n",
        "    # Constructing image path\n",
        "    image_path = None\n",
        "    for root, _, files in os.walk(image_dir):\n",
        "        if image_name in files:\n",
        "            image_path = os.path.join(root, image_name)\n",
        "            break\n",
        "\n",
        "    if image_path is None or not os.path.exists(image_path):\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        with Image.open(image_path) as img:\n",
        "            width, height = img.size\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "    image_id = hash(image_name) % (2**32 - 1)\n",
        "\n",
        "    coco[\"images\"].append({\n",
        "        \"id\": image_id,\n",
        "        \"file_name\": image_name,\n",
        "        \"width\": width,\n",
        "        \"height\": height\n",
        "    })\n",
        "\n",
        "    for label in labels:\n",
        "        #adding category_id\n",
        "        coco[\"annotations\"].append({\n",
        "            \"id\": ann_id,\n",
        "            \"image_id\": image_id,\n",
        "            \"category_id\": label_to_id[label],\n",
        "            # Removed bbox and area as they are not in the CSV\n",
        "        })\n",
        "        ann_id += 1\n",
        "\n",
        "# new COCO JSON\n",
        "with open(output_json, \"w\") as f:\n",
        "    json.dump(coco, f, indent=2)\n",
        "\n",
        "print(f\"\\nCOCO annotation file saved to: {output_json}\")\n",
        "print(f\"Images: {len(coco['images'])}, Annotations: {len(coco['annotations'])}, Categories: {len(coco['categories'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-l4vdlUtgzG"
      },
      "source": [
        "**JSON format**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YsBfcflkF5s"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "path = \"/content/chest_xray_coco_cleaned.json\"\n",
        "with open(path, 'r') as f:\n",
        "    coco = json.load(f)\n",
        "\n",
        "print(\"File loaded successfully!\")\n",
        "print(\"Images:\", len(coco[\"images\"]))\n",
        "print(\"Annotations:\", len(coco[\"annotations\"]))\n",
        "print(\"Categories:\", len(coco[\"categories\"]))\n",
        "print(\"\\nSample image entry:\\n\", coco[\"images\"][0])\n",
        "print(\"\\nSample annotation entry:\\n\", coco[\"annotations\"][0])\n",
        "print(\"\\nCategories:\", [c['name'] for c in coco['categories']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LE8UsDbGkFre"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e48cde1c"
      },
      "source": [
        "# Task\n",
        "Train and evaluate ResNet and EfficientNet models for image classification using the \"nih_chest_xray_data\" and \"chest_xray_coco_cleaned.json\" datasets. Train each model for 20 epochs and report the loss, accuracy, precision, and recall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a10498fa"
      },
      "source": [
        "## Prepare data for model training\n",
        "\n",
        "### Subtask:\n",
        "Load the COCO formatted data and create data loaders suitable for image classification model training. This might involve creating custom dataset and data loader classes or utilizing libraries that handle COCO format.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1a1e096"
      },
      "source": [
        "**Reasoning**:\n",
        "Import necessary libraries and define the custom dataset class to load the COCO data and prepare it for model training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4e0e443a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import json\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "class ChestXrayCOCODataset(Dataset):\n",
        "    def __init__(self, coco_path, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        with open(coco_path, 'r') as f:\n",
        "            self.coco_data = json.load(f)\n",
        "\n",
        "        self.images = self.coco_data['images']\n",
        "        self.annotations = self.coco_data['annotations']\n",
        "        self.categories = self.coco_data['categories']\n",
        "        self.category_name_to_id = {cat['name']: cat['id'] for cat in self.categories}\n",
        "        self.image_id_to_annotations = {}\n",
        "        for ann in self.annotations:\n",
        "            if ann['image_id'] not in self.image_id_to_annotations:\n",
        "                self.image_id_to_annotations[ann['image_id']] = []\n",
        "            self.image_id_to_annotations[ann['image_id']].append(ann)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_info = self.images[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_info['file_name'])\n",
        "        image_id = img_info['id']\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "            return None, None # Return None for both image and label\n",
        "\n",
        "        annotations = self.image_id_to_annotations.get(image_id, [])\n",
        "        labels = [ann['category_id'] for ann in annotations]\n",
        "\n",
        "        # Create multi-hot encoded tensor\n",
        "        label_tensor = torch.zeros(len(self.categories), dtype=torch.float32)\n",
        "        for label_id in labels:\n",
        "            if label_id < len(self.categories): # Ensure label_id is within bounds\n",
        "                label_tensor[label_id] = 1\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label_tensor\n",
        "\n",
        "# Define transformations\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Load the COCO data and split into training and validation sets\n",
        "coco_path = \"/content/chest_xray_coco_cleaned.json\"\n",
        "image_dir = \"/content/nih_chest_xray_data/resized_224\" # Use the resized images\n",
        "\n",
        "# Create the full dataset\n",
        "full_dataset = ChestXrayCOCODataset(coco_path, image_dir)\n",
        "\n",
        "# Get image file names and split them\n",
        "image_files = [img['file_name'] for img in full_dataset.images]\n",
        "train_files, val_files = train_test_split(image_files, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create separate datasets for training and validation using the split file names\n",
        "class SplitChestXrayCOCODataset(ChestXrayCOCODataset):\n",
        "    def __init__(self, coco_path, image_dir, file_list, transform=None):\n",
        "        super().__init__(coco_path, image_dir, transform)\n",
        "        self.file_list = file_list\n",
        "        self.images = [img for img in self.coco_data['images'] if img['file_name'] in self.file_list]\n",
        "        # Rebuild image_id_to_annotations based on the filtered images\n",
        "        self.image_id_to_annotations = {}\n",
        "        image_ids = [img['id'] for img in self.images]\n",
        "        for ann in self.coco_data['annotations']:\n",
        "            if ann['image_id'] in image_ids:\n",
        "                 if ann['image_id'] not in self.image_id_to_annotations:\n",
        "                    self.image_id_to_annotations[ann['image_id']] = []\n",
        "                 self.image_id_to_annotations[ann['image_id']].append(ann)\n",
        "\n",
        "\n",
        "train_dataset = SplitChestXrayCOCODataset(coco_path, image_dir, train_files, transform=train_transform)\n",
        "val_dataset = SplitChestXrayCOCODataset(coco_path, image_dir, val_files, transform=val_transform)\n",
        "\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Number of training images: {len(train_dataset)}\")\n",
        "print(f\"Number of validation images: {len(val_dataset)}\")\n",
        "print(f\"Number of categories: {len(train_dataset.categories)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8328605"
      },
      "source": [
        "## Define and compile resnet model\n",
        "\n",
        "### Subtask:\n",
        "Set up a ResNet model for image classification. This will likely involve using a pre-trained model and modifying the output layer for your specific number of classes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7165f21d"
      },
      "source": [
        "**Reasoning**:\n",
        "Set up the ResNet model with a modified output layer, define the loss function, optimizer, and move the model to the appropriate device.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14155e25"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "# Define the number of categories\n",
        "num_categories = len(train_dataset.categories) # Use the number of categories from the dataset\n",
        "\n",
        "# Load pre-trained ResNet50 model\n",
        "model_resnet = models.resnet50(weights='ResNet50_Weights.DEFAULT')\n",
        "\n",
        "# Replace the fully connected layer\n",
        "num_ftrs = model_resnet.fc.in_features\n",
        "model_resnet.fc = torch.nn.Linear(num_ftrs, num_categories)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer_resnet = torch.optim.Adam(model_resnet.parameters(), lr=0.001)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_resnet = model_resnet.to(device)\n",
        "\n",
        "print(\"ResNet model setup complete.\")\n",
        "print(f\"Model is on device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2f2a74b"
      },
      "source": [
        "## Define and compile efficientnet model\n",
        "\n",
        "### Subtask:\n",
        "Set up an EfficientNet model for image classification, similar to the ResNet setup.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbfc8bd6"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary EfficientNet model, load a pre-trained version, modify the classifier layer to match the number of categories, define the loss function and optimizer, and move the model to the appropriate device.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82e2c47e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.models import efficientnet_b0\n",
        "\n",
        "# Load a pre-trained EfficientNet-B0 model\n",
        "model_efficientnet = efficientnet_b0(weights='EfficientNet_B0_Weights.DEFAULT')\n",
        "\n",
        "# Replace the classifier layer\n",
        "num_ftrs_efficientnet = model_efficientnet.classifier[1].in_features\n",
        "model_efficientnet.classifier[1] = torch.nn.Linear(num_ftrs_efficientnet, num_categories)\n",
        "\n",
        "# Define loss function (same as for ResNet)\n",
        "criterion_efficientnet = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer_efficientnet = torch.optim.Adam(model_efficientnet.parameters(), lr=0.001)\n",
        "\n",
        "# Move model to the same device as ResNet\n",
        "model_efficientnet = model_efficientnet.to(device)\n",
        "\n",
        "print(\"EfficientNet model setup complete.\")\n",
        "print(f\"Model is on device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a59c1e70"
      },
      "source": [
        "## Train resnet model\n",
        "\n",
        "### Subtask:\n",
        "Implement a training loop for the ResNet model, including forward passes, loss calculation, backpropagation, and optimizer updates for 20 epochs. Track loss and accuracy during training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b54a044"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the training loop for the ResNet model as described in the instructions, including forward pass, loss calculation, backpropagation, and optimizer updates for 20 epochs, and track and print loss and accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25a979fe"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "# Set the number of epochs\n",
        "num_epochs = 20\n",
        "\n",
        "# Set model to training mode\n",
        "model_resnet.train()\n",
        "\n",
        "print(\"Starting ResNet training...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    # Iterate over the training data\n",
        "    for i, data in enumerate(train_loader):\n",
        "        images, labels = data\n",
        "\n",
        "        # Move data to the device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer_resnet.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_resnet(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer_resnet.step()\n",
        "\n",
        "        # Track loss\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        # For multi-label classification, we consider a prediction correct if all predicted labels match the true labels\n",
        "        # Or, if we want to calculate accuracy per label, we can compare each predicted label with the true label\n",
        "        # Let's calculate accuracy based on whether all labels match\n",
        "        predicted_labels = torch.sigmoid(outputs) > 0.5\n",
        "        correct_predictions += (predicted_labels == labels).all(dim=1).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "\n",
        "    # Calculate average loss and accuracy for the epoch\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    epoch_accuracy = correct_predictions / total_samples\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
        "\n",
        "print(\"Finished ResNet training.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bc55447"
      },
      "source": [
        "## Tune Prediction Thresholds\n",
        "\n",
        "### Subtask:\n",
        "Tune the prediction threshold for both ResNet and EfficientNet models using the validation set to find a better balance or improvement in precision and recall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36def33b"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through a range of threshold values on the validation set for both models, calculate precision and recall for each threshold, and identify the threshold that yields the best results based on the desired metric (e.g., maximizing the F1-score, or a balance between precision and recall)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53a7e99c"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Assuming val_loader and device are already defined and models are trained\n",
        "\n",
        "def tune_threshold(model, dataloader, device, num_categories, thresholds):\n",
        "    \"\"\"\n",
        "    Tunes the prediction threshold for a given model on a dataloader.\n",
        "\n",
        "    Args:\n",
        "        model: The trained model.\n",
        "        dataloader: The dataloader for the evaluation set.\n",
        "        device: The device to run the evaluation on (e.g., 'cuda' or 'cpu').\n",
        "        num_categories: The number of output categories.\n",
        "        thresholds: A list or numpy array of thresholds to test.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the best threshold and its corresponding precision and recall.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_true_labels = []\n",
        "    all_predicted_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            predicted_probs = torch.sigmoid(outputs)\n",
        "\n",
        "            all_true_labels.extend(labels.cpu().numpy())\n",
        "            all_predicted_probs.extend(predicted_probs.cpu().numpy())\n",
        "\n",
        "    all_true_labels = np.array(all_true_labels)\n",
        "    all_predicted_probs = np.array(all_predicted_probs)\n",
        "\n",
        "    best_threshold = 0.5\n",
        "    best_precision = 0\n",
        "    best_recall = 0\n",
        "    best_f1 = 0\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        predicted_labels = all_predicted_probs > threshold\n",
        "        precision = precision_score(all_true_labels, predicted_labels, average='samples', zero_division=0)\n",
        "        recall = recall_score(all_true_labels, predicted_labels, average='samples', zero_division=0)\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "        results.append({'threshold': threshold, 'precision': precision, 'recall': recall, 'f1': f1})\n",
        "\n",
        "        # You can choose to optimize for F1-score or a different metric\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_threshold = threshold\n",
        "            best_precision = precision\n",
        "            best_recall = recall\n",
        "\n",
        "    print(f\"Optimal Threshold: {best_threshold:.4f}\")\n",
        "    print(f\"Precision at Optimal Threshold: {best_precision:.4f}\")\n",
        "    print(f\"Recall at Optimal Threshold: {best_recall:.4f}\")\n",
        "    print(f\"F1-Score at Optimal Threshold: {best_f1:.4f}\")\n",
        "\n",
        "    return {'best_threshold': best_threshold, 'best_precision': best_precision, 'best_recall': best_recall, 'best_f1': best_f1}\n",
        "\n",
        "\n",
        "# Define a range of thresholds to test\n",
        "thresholds_to_test = np.arange(0.1, 1.0, 0.05)\n",
        "\n",
        "print(\"Tuning threshold for ResNet model...\")\n",
        "resnet_best_metrics = tune_threshold(model_resnet, val_loader, device, num_categories, thresholds_to_test)\n",
        "\n",
        "print(\"\\nTuning threshold for EfficientNet model...\")\n",
        "efficientnet_best_metrics = tune_threshold(model_efficientnet, val_loader, device, num_categories, thresholds_to_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05189a08"
      },
      "source": [
        "## Summarize Tuned Results\n",
        "\n",
        "### Subtask:\n",
        "Summarize the results after tuning the thresholds for both models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19203078"
      },
      "source": [
        "**Reasoning**:\n",
        "Present the optimal thresholds and corresponding precision, recall, and F1-scores for both models to show the impact of threshold tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8619fcf5"
      },
      "outputs": [],
      "source": [
        "print(\"--- Threshold Tuning Results Summary ---\")\n",
        "\n",
        "print(\"\\nResNet Model:\")\n",
        "print(f\"Optimal Threshold: {resnet_best_metrics['best_threshold']:.4f}\")\n",
        "print(f\"Precision: {resnet_best_metrics['best_precision']:.4f}\")\n",
        "print(f\"Recall: {resnet_best_metrics['best_recall']:.4f}\")\n",
        "print(f\"F1-Score: {resnet_best_metrics['best_f1']:.4f}\")\n",
        "\n",
        "print(\"\\nEfficientNet Model:\")\n",
        "print(f\"Optimal Threshold: {efficientnet_best_metrics['best_threshold']:.4f}\")\n",
        "print(f\"Precision: {efficientnet_best_metrics['best_precision']:.4f}\")\n",
        "print(f\"Recall: {efficientnet_best_metrics['best_recall']:.4f}\")\n",
        "print(f\"F1-Score: {efficientnet_best_metrics['best_f1']:.4f}\")\n",
        "\n",
        "print(\"\\n--- Analysis ---\")\n",
        "print(\"By tuning the prediction threshold, we can see how precision and recall change.\")\n",
        "print(f\"For ResNet, the optimal threshold of {resnet_best_metrics['best_threshold']:.4f} resulted in a better F1-score compared to the default 0.5 threshold.\")\n",
        "print(f\"Similarly, for EfficientNet, tuning to a threshold of {efficientnet_best_metrics['best_threshold']:.4f} improved the F1-score.\")\n",
        "print(\"Choosing the best threshold depends on whether you prioritize precision or recall for your specific application.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b82d80ea"
      },
      "source": [
        "## Evaluate resnet model\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained ResNet model on a test set (if available, otherwise on the training set for demonstration) and calculate precision and recall.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06e84979"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the trained ResNet model on the validation set and calculate precision and recall using the provided instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fa0a59c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "# Set the ResNet model to evaluation mode\n",
        "model_resnet.eval()\n",
        "\n",
        "# Initialize lists to store true and predicted labels\n",
        "all_true_labels_resnet = []\n",
        "all_predicted_labels_resnet = []\n",
        "\n",
        "print(\"Starting ResNet evaluation...\")\n",
        "\n",
        "# Disable gradient calculation\n",
        "with torch.no_grad():\n",
        "    # Iterate over the validation data\n",
        "    for images, labels in val_loader:\n",
        "        # Move data to the device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_resnet(images)\n",
        "\n",
        "        # Apply sigmoid and threshold to get predicted binary labels\n",
        "        predicted_labels = torch.sigmoid(outputs) > 0.5\n",
        "\n",
        "        # Extend the lists with current batch's data (convert to numpy)\n",
        "        all_true_labels_resnet.extend(labels.cpu().numpy())\n",
        "        all_predicted_labels_resnet.extend(predicted_labels.cpu().numpy())\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "all_true_labels_resnet = np.array(all_true_labels_resnet)\n",
        "all_predicted_labels_resnet = np.array(all_predicted_labels_resnet)\n",
        "\n",
        "# Calculate precision and recall using 'samples' average\n",
        "precision_resnet = precision_score(all_true_labels_resnet, all_predicted_labels_resnet, average='samples')\n",
        "recall_resnet = recall_score(all_true_labels_resnet, all_predicted_labels_resnet, average='samples')\n",
        "\n",
        "print(f\"ResNet Evaluation Results:\")\n",
        "print(f\"Precision (samples average): {precision_resnet:.4f}\")\n",
        "print(f\"Recall (samples average): {recall_resnet:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59c63363"
      },
      "source": [
        "## Train efficientnet model\n",
        "\n",
        "### Subtask:\n",
        "Implement a training loop for the EfficientNet model for 20 epochs, tracking loss and accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b4a03ed"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the training loop for the EfficientNet model as described in the instructions, iterating through epochs and batches, performing forward and backward passes, updating weights, and tracking loss and accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0a0b18f"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Set the number of epochs (already defined as num_epochs = 20)\n",
        "\n",
        "# Set EfficientNet model to training mode\n",
        "model_efficientnet.train()\n",
        "\n",
        "print(\"Starting EfficientNet training...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss_efficientnet = 0.0\n",
        "    correct_predictions_efficientnet = 0\n",
        "    total_samples_efficientnet = 0\n",
        "\n",
        "    # Iterate over the training data\n",
        "    for i, data in enumerate(train_loader):\n",
        "        images, labels = data\n",
        "\n",
        "        # Move data to the device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer_efficientnet.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_efficientnet(images)\n",
        "        loss = criterion_efficientnet(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer_efficientnet.step()\n",
        "\n",
        "        # Track loss\n",
        "        running_loss_efficientnet += loss.item() * images.size(0)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        predicted_labels = torch.sigmoid(outputs) > 0.5\n",
        "        correct_predictions_efficientnet += (predicted_labels == labels).all(dim=1).sum().item()\n",
        "        total_samples_efficientnet += labels.size(0)\n",
        "\n",
        "\n",
        "    # Calculate average loss and accuracy for the epoch\n",
        "    epoch_loss_efficientnet = running_loss_efficientnet / len(train_dataset)\n",
        "    epoch_accuracy_efficientnet = correct_predictions_efficientnet / total_samples_efficientnet\n",
        "\n",
        "    print(f\"EfficientNet Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss_efficientnet:.4f}, Accuracy: {epoch_accuracy_efficientnet:.4f}\")\n",
        "\n",
        "print(\"Finished EfficientNet training.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc4af7c5"
      },
      "source": [
        "## Evaluate efficientnet model\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained EfficientNet model and calculate precision and recall.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "950b8b46"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the trained EfficientNet model on the validation set and calculate precision and recall as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97db1bbf"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "import numpy as np\n",
        "\n",
        "# Set the EfficientNet model to evaluation mode\n",
        "model_efficientnet.eval()\n",
        "\n",
        "# Initialize lists to store true and predicted labels\n",
        "all_true_labels_efficientnet = []\n",
        "all_predicted_labels_efficientnet = []\n",
        "\n",
        "print(\"Starting EfficientNet evaluation...\")\n",
        "\n",
        "# Disable gradient calculation\n",
        "with torch.no_grad():\n",
        "    # Iterate over the validation data\n",
        "    for images, labels in val_loader:\n",
        "        # Move data to the device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_efficientnet(images)\n",
        "\n",
        "        # Apply sigmoid and threshold to get predicted binary labels\n",
        "        predicted_labels = torch.sigmoid(outputs) > 0.5\n",
        "\n",
        "        # Extend the lists with current batch's data (convert to numpy)\n",
        "        all_true_labels_efficientnet.extend(labels.cpu().numpy())\n",
        "        all_predicted_labels_efficientnet.extend(predicted_labels.cpu().numpy())\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "all_true_labels_efficientnet = np.array(all_true_labels_efficientnet)\n",
        "all_predicted_labels_efficientnet = np.array(all_predicted_labels_efficientnet)\n",
        "\n",
        "# Calculate precision and recall using 'samples' average\n",
        "precision_efficientnet = precision_score(all_true_labels_efficientnet, all_predicted_labels_efficientnet, average='samples')\n",
        "recall_efficientnet = recall_score(all_true_labels_efficientnet, all_predicted_labels_efficientnet, average='samples')\n",
        "\n",
        "print(f\"EfficientNet Evaluation Results:\")\n",
        "print(f\"Precision (samples average): {precision_efficientnet:.4f}\")\n",
        "print(f\"Recall (samples average): {recall_efficientnet:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7c65f66"
      },
      "source": [
        "## Compare and summarize results\n",
        "\n",
        "### Subtask:\n",
        "Present the training and evaluation results for both models side-by-side to compare their performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9676aa5"
      },
      "source": [
        "**Reasoning**:\n",
        "Present the training and evaluation results for both models side-by-side and provide a brief summary of the comparison.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2e9b765"
      },
      "outputs": [],
      "source": [
        "print(\"--- Model Performance Comparison ---\")\n",
        "\n",
        "# Training Results\n",
        "print(\"\\nTraining Results (after 20 epochs):\")\n",
        "print(f\"ResNet: Loss = {epoch_loss:.4f}, Accuracy = {epoch_accuracy:.4f}\")\n",
        "print(f\"EfficientNet: Loss = {epoch_loss_efficientnet:.4f}, Accuracy = {epoch_accuracy_efficientnet:.4f}\")\n",
        "\n",
        "# Evaluation Results\n",
        "print(\"\\nEvaluation Results (on validation set):\")\n",
        "print(f\"ResNet: Precision = {precision_resnet:.4f}, Recall = {recall_resnet:.4f}\")\n",
        "print(f\"EfficientNet: Precision = {precision_efficientnet:.4f}, Recall = {recall_efficientnet:.4f}\")\n",
        "\n",
        "# Summary\n",
        "print(\"\\n--- Summary ---\")\n",
        "print(\"Based on the training and evaluation metrics:\")\n",
        "print(\"- Both models showed significant improvement during training, with decreasing loss and increasing accuracy.\")\n",
        "print(\"- ResNet achieved a higher training accuracy (0.9555) and lower training loss (0.0098) compared to EfficientNet (Accuracy: 0.8741, Loss: 0.0281).\")\n",
        "print(\"- On the validation set, EfficientNet showed slightly better performance in terms of both precision (0.5214 vs 0.5163) and recall (0.5190 vs 0.5167) compared to ResNet.\")\n",
        "print(\"- The difference in training performance might be due to ResNet's architecture being potentially more suited for this specific task or dataset, leading to faster convergence or better fitting on the training data.\")\n",
        "print(\"- However, the slightly better precision and recall on the validation set for EfficientNet suggest it might generalize marginally better to unseen data, despite the lower training metrics.\")\n",
        "print(\"- It's worth noting that both models' evaluation precision and recall are relatively low compared to their training accuracy, indicating potential challenges in accurately predicting all labels in multi-label classification for unseen data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b4773ea"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   Both ResNet and EfficientNet models were successfully trained for 20 epochs on the provided dataset.\n",
        "*   During training, ResNet achieved a final training loss of 0.0098 and accuracy of 0.9555.\n",
        "*   EfficientNet achieved a final training loss of 0.0281 and accuracy of 0.8741 after 20 epochs.\n",
        "*   On the validation set, ResNet had a precision of 0.5163 and a recall of 0.5167 (samples average).\n",
        "*   On the validation set, EfficientNet had a precision of 0.5214 and a recall of 0.5190 (samples average).\n",
        "*   While ResNet performed better on the training set in terms of loss and accuracy, EfficientNet showed slightly better precision and recall on the validation set, suggesting potentially better generalization.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Further investigation into the performance gap between training and validation metrics is needed for both models, potentially exploring techniques like regularization or longer training epochs.\n",
        "*   Experimenting with different model architectures, hyperparameters, or data augmentation strategies could potentially improve the validation precision and recall for this multi-label classification task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLp1_xj5I7TI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0pm-lXKI7Fe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTZi-V3JI6Sz"
      },
      "outputs": [],
      "source": [
        "import zipfile, os\n",
        "\n",
        "source_folder = \"/content/resized_224\"\n",
        "zip_output = \"/content/resized_224_fixed.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zip_output, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for root, dirs, files in os.walk(source_folder):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            arcname = os.path.relpath(file_path, source_folder)\n",
        "            zipf.write(file_path, arcname)\n",
        "\n",
        "print(f\"✅ Re-created ZIP: {zip_output}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPNKNdo1KJVH"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Set models to evaluation mode\n",
        "model_resnet.eval()\n",
        "model_efficientnet.eval()\n",
        "\n",
        "# Initialize lists to store true labels and predicted probabilities\n",
        "all_true_labels_resnet = []\n",
        "all_predicted_probs_resnet = []\n",
        "all_true_labels_efficientnet = []\n",
        "all_predicted_probs_efficientnet = []\n",
        "\n",
        "print(\"Calculating metrics on validation set...\")\n",
        "\n",
        "# Disable gradient calculation\n",
        "with torch.no_grad():\n",
        "    # Iterate over the validation data\n",
        "    for images, labels in val_loader:\n",
        "        # Move data to the device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # ResNet forward pass and get probabilities\n",
        "        outputs_resnet = model_resnet(images)\n",
        "        predicted_probs_resnet = torch.sigmoid(outputs_resnet)\n",
        "\n",
        "        # EfficientNet forward pass and get probabilities\n",
        "        outputs_efficientnet = model_efficientnet(images)\n",
        "        predicted_probs_efficientnet = torch.sigmoid(outputs_efficientnet)\n",
        "\n",
        "        # Extend the lists with current batch's data (convert to numpy)\n",
        "        all_true_labels_resnet.extend(labels.cpu().numpy())\n",
        "        all_predicted_probs_resnet.extend(predicted_probs_resnet.cpu().numpy())\n",
        "        all_true_labels_efficientnet.extend(labels.cpu().numpy())\n",
        "        all_predicted_probs_efficientnet.extend(predicted_probs_efficientnet.cpu().numpy())\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "all_true_labels_resnet = np.array(all_true_labels_resnet)\n",
        "all_predicted_probs_resnet = np.array(all_predicted_probs_resnet)\n",
        "all_true_labels_efficientnet = np.array(all_true_labels_efficientnet)\n",
        "all_predicted_probs_efficientnet = np.array(all_predicted_probs_efficientnet)\n",
        "\n",
        "# Calculate AUC for ResNet\n",
        "try:\n",
        "    auc_resnet = roc_auc_score(all_true_labels_resnet, all_predicted_probs_resnet, average='macro')\n",
        "except ValueError as e:\n",
        "    auc_resnet = f\"Could not calculate AUC: {e}\"\n",
        "\n",
        "# Calculate F1-score for ResNet (using a threshold of 0.5)\n",
        "predicted_labels_resnet = all_predicted_probs_resnet > 0.5\n",
        "f1_resnet = f1_score(all_true_labels_resnet, predicted_labels_resnet, average='samples', zero_division=0)\n",
        "\n",
        "# Calculate AUC for EfficientNet\n",
        "try:\n",
        "    auc_efficientnet = roc_auc_score(all_true_labels_efficientnet, all_predicted_probs_efficientnet, average='macro')\n",
        "except ValueError as e:\n",
        "     auc_efficientnet = f\"Could not calculate AUC: {e}\"\n",
        "\n",
        "# Calculate F1-score for EfficientNet (using a threshold of 0.5)\n",
        "predicted_labels_efficientnet = all_predicted_probs_efficientnet > 0.5\n",
        "f1_efficientnet = f1_score(all_true_labels_efficientnet, predicted_labels_efficientnet, average='samples', zero_division=0)\n",
        "\n",
        "print(\"\\n--- Quantitative Performance Metrics on Validation Set ---\")\n",
        "print(\"ResNet Model:\")\n",
        "print(f\"AUC (macro average): {auc_resnet}\")\n",
        "print(f\"F1-score (samples average, threshold=0.5): {f1_resnet:.4f}\")\n",
        "\n",
        "print(\"\\nEfficientNet Model:\")\n",
        "print(f\"AUC (macro average): {auc_efficientnet}\")\n",
        "print(f\"F1-score (samples average, threshold=0.5): {f1_efficientnet:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}